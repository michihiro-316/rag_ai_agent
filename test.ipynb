{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38aee237",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "client = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash-001\",\n",
    "    project=\"dify-chatbot-469403\",\n",
    "    location=\"us-central1\"\n",
    ")\n",
    "\n",
    "response = client.invoke([\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"ã“ã‚“ã«ã¡ã¯ï¼ç§ã¯ã‚¸ãƒ§ãƒ³ã¨ã„ã„ã¾ã™ã€‚\"}\n",
    "])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0b9d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "client = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash-001\",\n",
    "    project=\"dify-chatbot-469403\",\n",
    "    location=\"us-central1\",\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "response = client.invoke([\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"ã“ã‚“ã«ã¡ã¯ï¼ç§ã¯ã‚¸ãƒ§ãƒ³ã¨ã„ã„ã¾ã™ã€‚\"},\n",
    "\n",
    "])\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b9748c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "client = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash-001\",\n",
    "    project=\"dify-chatbot-469403\",\n",
    "    location=\"us-central1\",\n",
    "    response_mime_type=\"application/json\"\n",
    ")\n",
    "\n",
    "response = client.invoke([\n",
    "    {\"role\": \"system\", \"content\": \"äººç‰©ä¸€è¦§ã‚’æ¬¡ã®JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚[{\\\"name\\\": \\\"\\\", \\\"age\\\": \\\"\\\", \\\"city\\\": \\\"\\\"}]\"},\n",
    "    {\"role\": \"user\", \"content\": \"ä»¥ä¸‹ã®äººç‰©ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚ä½è—¤å¤ªéƒ50æ­³ã€æ±äº¬éƒ½å‡ºèº«ã§ã™ã€‚æ™®æ®µã¯ä¼šç¤¾ã«å‹¤ã‚ã¦ã¾ã™ã€‚éˆ´æœ¨èŠ±å­30æ­³ã€å¤§é˜ªåºœå‡ºèº«ã§ã™ã€‚ãƒ•ãƒªãƒ¼ãƒ©ãƒ³ã‚¹ã§åƒã„ã¦ã„ã¾ã™ã€‚\"},\n",
    "])\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c059825e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# Function Calling å…¥é–€\n",
    "# ====================================\n",
    "# AIãŒã€Œã©ã®é–¢æ•°ã‚’å‘¼ã¶ã¹ãã‹ã€ã‚’åˆ¤æ–­ã—ã€å¼•æ•°ã‚’ç”Ÿæˆã™ã‚‹æ©Ÿèƒ½\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "# 1. ãƒ„ãƒ¼ãƒ«ï¼ˆé–¢æ•°ï¼‰ã‚’å®šç¾©\n",
    "@tool\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"æŒ‡å®šã—ãŸéƒ½å¸‚ã®å¤©æ°—ã‚’å–å¾—ã™ã‚‹\"\"\"\n",
    "    # å®Ÿéš›ã¯APIã‚’å‘¼ã¶ãŒã€ã“ã“ã§ã¯ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿\n",
    "    weather_data = {\n",
    "        \"æ±äº¬\": \"æ™´ã‚Œ 25â„ƒ\",\n",
    "        \"å¤§é˜ª\": \"æ›‡ã‚Š 22â„ƒ\",\n",
    "        \"åå¤å±‹\": \"é›¨ 18â„ƒ\"\n",
    "    }\n",
    "    return weather_data.get(city, f\"{city}ã®å¤©æ°—æƒ…å ±ã¯ã‚ã‚Šã¾ã›ã‚“\")\n",
    "\n",
    "@tool\n",
    "def calculate(expression: str) -> str:\n",
    "    \"\"\"æ•°å¼ã‚’è¨ˆç®—ã™ã‚‹\"\"\"\n",
    "    try:\n",
    "        result = eval(expression)\n",
    "        return f\"è¨ˆç®—çµæœ: {result}\"\n",
    "    except:\n",
    "        return \"è¨ˆç®—ã§ãã¾ã›ã‚“ã§ã—ãŸ\"\n",
    "\n",
    "# 2. LLMã«ãƒ„ãƒ¼ãƒ«ã‚’ãƒã‚¤ãƒ³ãƒ‰\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash-001\",\n",
    "    project=\"dify-chatbot-469403\",\n",
    "    location=\"us-central1\"\n",
    ")\n",
    "\n",
    "llm_with_tools = llm.bind_tools([get_weather, calculate])\n",
    "\n",
    "# 3. è³ªå•ã—ã¦ã¿ã‚‹\n",
    "response = llm_with_tools.invoke(\"å¤§åˆ†çœŒã®å¤©æ°—ã¯ï¼Ÿ\")\n",
    "#ä»Šã¾ã§ã¯response = llm.invoke{[\n",
    "#    {\"role\": \"system\", \"content\": \"äººç‰©ä¸€è¦§ã‚’æ¬¡ã®JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚[{\\\"name\\\": \\\"\\\", \\\"age\\\": \\\"\\\", \\\"city\\\": \\\"\\\"}]\"},\n",
    "#    {\"role\": \"user\", \"content\": \"ä»¥ä¸‹ã®äººç‰©ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚ä½è—¤å¤ªéƒ50æ­³ã€æ±äº¬éƒ½å‡ºèº«ã§ã™ã€‚æ™®æ®µã¯ä¼šç¤¾ã«å‹¤ã‚ã¦ã¾ã™ã€‚éˆ´æœ¨èŠ±å­30æ­³ã€å¤§é˜ªåºœå‡ºèº«ã§ã™ã€‚ãƒ•ãƒªãƒ¼ãƒ©ãƒ³ã‚¹ã§åƒã„ã¦ã„ã¾ã™ã€‚\"},\n",
    "#]}\n",
    "\n",
    "\n",
    "print(\"=== AIã®åˆ¤æ–­ ===\")\n",
    "print(f\"å‘¼ã³å‡ºã™ãƒ„ãƒ¼ãƒ«: {response.tool_calls}\")\n",
    "print()\n",
    "\n",
    "# 4. ãƒ„ãƒ¼ãƒ«ã‚’å®Ÿéš›ã«å®Ÿè¡Œ\n",
    "if response.tool_calls:\n",
    "    tool_call = response.tool_calls[0]\n",
    "    tool_name = tool_call[\"name\"]\n",
    "    tool_args = tool_call[\"args\"]\n",
    "\n",
    "    print(f\"=== ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œ ===\")\n",
    "    print(f\"ãƒ„ãƒ¼ãƒ«å: {tool_name}\")\n",
    "    print(f\"å¼•æ•°: {tool_args}\")\n",
    "\n",
    "    # ãƒ„ãƒ¼ãƒ«ã‚’å®Ÿè¡Œ\n",
    "    if tool_name == \"get_weather\":\n",
    "        result = get_weather.invoke(tool_args)\n",
    "    elif tool_name == \"calculate\":\n",
    "        result = calculate.invoke(tool_args)\n",
    "\n",
    "    print(f\"çµæœ: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f581bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def get_current_weather(location,unint=\"fahrenheit\"):\n",
    "    if \"tokyo\" in location.lower():\n",
    "        return {\n",
    "            \"location\": \"Tokyo\",\n",
    "            \"temperature\": \"22\",\n",
    "            \"unit\": unint,\n",
    "            \"forecast\": \"sunny\"\n",
    "        }\n",
    "    elif \"osaka\" in location.lower():\n",
    "        return {\n",
    "            \"location\": \"Osaka\",\n",
    "            \"temperature\": \"25\",\n",
    "            \"unit\": unint,\n",
    "            \"forecast\": \"cloudy\"\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"location\": location,\n",
    "            \"temperature\": \"unknown\",\n",
    "            \"unit\": unint,\n",
    "            \"forecast\": \"unknown\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e5e8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# Function Calling Ã— SQL\n",
    "# ====================================\n",
    "# è‡ªç„¶è¨€èªã‹ã‚‰SQLã‚’ç”Ÿæˆãƒ»å®Ÿè¡Œã™ã‚‹\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.tools import tool\n",
    "import sqlite3\n",
    "\n",
    "# 1. ã‚¤ãƒ³ãƒ¡ãƒ¢ãƒªDBã‚’ä½œæˆã—ã¦ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’æŠ•å…¥\n",
    "conn = sqlite3.connect(\":memory:\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "    CREATE TABLE users (\n",
    "        id INTEGER PRIMARY KEY,\n",
    "        name TEXT,\n",
    "        age INTEGER,\n",
    "        city TEXT,\n",
    "        department TEXT\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿\n",
    "sample_users = [\n",
    "    (\"å±±ç”°å¤ªéƒ\", 30, \"æ±äº¬\", \"å–¶æ¥­éƒ¨\"),\n",
    "    (\"éˆ´æœ¨èŠ±å­\", 25, \"å¤§é˜ª\", \"é–‹ç™ºéƒ¨\"),\n",
    "    (\"ä½è—¤æ¬¡éƒ\", 28, \"åå¤å±‹\", \"å–¶æ¥­éƒ¨\"),\n",
    "    (\"ç”°ä¸­ç¾å’²\", 35, \"æ±äº¬\", \"äººäº‹éƒ¨\"),\n",
    "    (\"é«˜æ©‹å¥ä¸€\", 42, \"ç¦å²¡\", \"é–‹ç™ºéƒ¨\"),\n",
    "    (\"ä¼Šè—¤ã•ãã‚‰\", 23, \"æ±äº¬\", \"é–‹ç™ºéƒ¨\"),\n",
    "]\n",
    "\n",
    "cursor.executemany(\"INSERT INTO users (name, age, city, department) VALUES (?, ?, ?, ?)\", sample_users)\n",
    "conn.commit()\n",
    "\n",
    "print(\"=== ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä½œæˆå®Œäº† ===\")\n",
    "print(\"ãƒ†ãƒ¼ãƒ–ãƒ«: users (id, name, age, city, department)\")\n",
    "print(f\"ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(sample_users)}ä»¶\")\n",
    "print()\n",
    "\n",
    "# å…¨ãƒ‡ãƒ¼ã‚¿è¡¨ç¤º\n",
    "cursor.execute(\"SELECT * FROM users\")\n",
    "for row in cursor.fetchall():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dxv1lsj1zi",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. SQLã‚’å®Ÿè¡Œã™ã‚‹ãƒ„ãƒ¼ãƒ«ã‚’å®šç¾©\n",
    "@tool\n",
    "def execute_sql(query: str) -> str:\n",
    "    \"\"\"\n",
    "    SQLã‚¯ã‚¨ãƒªã‚’å®Ÿè¡Œã—ã¦ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰æƒ…å ±ã‚’å–å¾—ã™ã‚‹ã€‚\n",
    "\n",
    "    ãƒ†ãƒ¼ãƒ–ãƒ«: users\n",
    "    ã‚«ãƒ©ãƒ : id, name(åå‰), age(å¹´é½¢), city(éƒ½å¸‚), department(éƒ¨ç½²)\n",
    "\n",
    "    ä¾‹:\n",
    "    - SELECT * FROM users WHERE city = 'æ±äº¬'\n",
    "    - SELECT * FROM users WHERE age >= 30\n",
    "    - SELECT department, COUNT(*) FROM users GROUP BY department\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cursor.execute(query)\n",
    "        results = cursor.fetchall()\n",
    "        if results:\n",
    "            columns = [desc[0] for desc in cursor.description]\n",
    "            output = f\"ã‚«ãƒ©ãƒ : {columns}\\n\"\n",
    "            for row in results:\n",
    "                output += f\"{row}\\n\"\n",
    "            return output\n",
    "        return \"è©²å½“ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ\"\n",
    "    except Exception as e:\n",
    "        return f\"ã‚¨ãƒ©ãƒ¼: {e}\"\n",
    "\n",
    "# 3. LLMã«ãƒ„ãƒ¼ãƒ«ã‚’ãƒã‚¤ãƒ³ãƒ‰\n",
    "llm_sql = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash-001\",\n",
    "    project=\"dify-chatbot-469403\",\n",
    "    location=\"us-central1\"\n",
    ")\n",
    "\n",
    "llm_with_sql = llm_sql.bind_tools([execute_sql])\n",
    "\n",
    "print(\"=== SQLãƒ„ãƒ¼ãƒ«è¨­å®šå®Œäº† ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wu77fpi63vs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. è‡ªç„¶è¨€èªã§è³ªå•ã—ã¦ã¿ã‚‹\n",
    "def ask_database(question: str):\n",
    "    \"\"\"è‡ªç„¶è¨€èªã§DBã«è³ªå•ã™ã‚‹\"\"\"\n",
    "    print(f\"è³ªå•: {question}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    response = llm_with_sql.invoke(question)\n",
    "\n",
    "    if response.tool_calls:\n",
    "        tool_call = response.tool_calls[0]\n",
    "        sql_query = tool_call[\"args\"][\"query\"]\n",
    "\n",
    "        print(f\"ç”Ÿæˆã•ã‚ŒãŸSQL: {sql_query}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        result = execute_sql.invoke({\"query\": sql_query})\n",
    "        print(f\"çµæœ:\\n{result}\")\n",
    "    else:\n",
    "        print(f\"AIã®å›ç­”: {response.content}\")\n",
    "\n",
    "    print(\"=\" * 50)\n",
    "    print()\n",
    "\n",
    "# ã„ãã¤ã‹ã®è³ªå•ã‚’è©¦ã™\n",
    "ask_database(\"æ±äº¬ã«ä½ã‚“ã§ã„ã‚‹äººã‚’å…¨å“¡æ•™ãˆã¦\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ipasf2q0v5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä»–ã®è³ªå•ã‚‚è©¦ã—ã¦ã¿ã‚ˆã†\n",
    "ask_database(\"30æ­³ä»¥ä¸Šã®ç¤¾å“¡ã¯èª°ï¼Ÿ\")\n",
    "ask_database(\"éƒ¨ç½²ã”ã¨ã®äººæ•°ã‚’æ•™ãˆã¦\")\n",
    "ask_database(\"é–‹ç™ºéƒ¨ã§ä¸€ç•ªè‹¥ã„äººã¯ï¼Ÿ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "af837a6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ã¯ã„ã€ã‚«ãƒ¬ãƒ¼ãƒ©ã‚¤ã‚¹ã®ãƒ¬ã‚·ãƒ”ã§ã™ã€‚åŸºæœ¬ã®ãƒ¬ã‚·ãƒ”ã‹ã‚‰ã‚¢ãƒ¬ãƒ³ã‚¸ã¾ã§ã”ç´¹ä»‹ã—ã¾ã™ã€‚\\n\\n**åŸºæœ¬ã®ã‚«ãƒ¬ãƒ¼ãƒ©ã‚¤ã‚¹**\\n\\n**ææ–™ (4ï½5äººåˆ†)**\\n\\n*   **è‚‰:**\\n    *   ç‰›è‚‰ã€è±šè‚‰ã€é¶è‚‰ãªã© (ã‚«ãƒ¬ãƒ¼ç”¨)ï¼š300ï½400g\\n*   **é‡èœ:**\\n    *   ç‰ã­ãï¼š2å€‹ (è–„åˆ‡ã‚Š)\\n    *   ã˜ã‚ƒãŒã„ã‚‚ï¼š2ï½3å€‹ (ä¹±åˆ‡ã‚Š)\\n    *   ã«ã‚“ã˜ã‚“ï¼š1æœ¬ (ä¹±åˆ‡ã‚Š)\\n*   **ãã®ä»–:**\\n    *   ã‚«ãƒ¬ãƒ¼ãƒ«ãƒ¼ï¼š1ç®± (8ï½12çš¿åˆ†)\\n    *   ã‚µãƒ©ãƒ€æ²¹ï¼šå¤§ã•ã˜2\\n    *   æ°´ï¼šãƒ«ãƒ¼ã®ç®±ã®è¡¨ç¤ºã«å¾“ã†\\n    *   ã”é£¯ï¼šé©é‡\\n\\n**ä½œã‚Šæ–¹**\\n\\n1.  **ä¸‹æº–å‚™:**\\n    *   è‚‰ã¯å¤§ãã‘ã‚Œã°é£Ÿã¹ã‚„ã™ã„å¤§ãã•ã«åˆ‡ã‚‹ã€‚\\n    *   é‡èœã¯ãã‚Œãã‚Œçš®ã‚’ã‚€ãã€ä¹±åˆ‡ã‚Šã«ã™ã‚‹ã€‚ç‰ã­ãã¯è–„åˆ‡ã‚Šã«ã™ã‚‹ã€‚\\n2.  **ç‚’ã‚:**\\n    *   é‹ã«ã‚µãƒ©ãƒ€æ²¹ã‚’ç†±ã—ã€ç‰ã­ãã‚’ç‚’ã‚ã‚‹ã€‚ç„¦ã’ä»˜ã‹ãªã„ã‚ˆã†ã«å¼±ç«ï½ä¸­ç«ã§ã€é£´è‰²ã«ãªã‚‹ã¾ã§ã˜ã£ãã‚Šç‚’ã‚ã‚‹ (ç´„10ï½15åˆ†)ã€‚\\n    *   è‚‰ã‚’åŠ ãˆã¦ç‚’ã‚ã€è¡¨é¢ã®è‰²ãŒå¤‰ã‚ã£ãŸã‚‰ã€ã˜ã‚ƒãŒã„ã‚‚ã€ã«ã‚“ã˜ã‚“ã‚’åŠ ãˆã¦ã•ã‚‰ã«ç‚’ã‚ã‚‹ã€‚\\n3.  **ç…®è¾¼ã¿:**\\n    *   é‡èœå…¨ä½“ã«æ²¹ãŒå›ã£ãŸã‚‰ã€ãƒ«ãƒ¼ã®ç®±ã«è¡¨ç¤ºã•ã‚Œã¦ã„ã‚‹åˆ†é‡ã®æ°´ã‚’åŠ ãˆã¦æ²¸é¨°ã•ã›ã‚‹ã€‚\\n    *   ã‚¢ã‚¯ãŒå‡ºã¦ããŸã‚‰ã™ãã„å–ã‚Šã€å¼±ç«ï½ä¸­ç«ã§é‡èœãŒæŸ”ã‚‰ã‹ããªã‚‹ã¾ã§ç…®è¾¼ã‚€ (ç´„15ï½20åˆ†)ã€‚\\n4.  **ãƒ«ãƒ¼ã‚’æº¶ã‹ã™:**\\n    *   ã„ã£ãŸã‚“ç«ã‚’æ­¢ã‚ã€ã‚«ãƒ¬ãƒ¼ãƒ«ãƒ¼ã‚’å‰²ã‚Šå…¥ã‚Œã¦æº¶ã‹ã™ã€‚\\n    *   å†ã³å¼±ç«ã§ã¨ã‚ã¿ãŒã¤ãã¾ã§ç…®è¾¼ã‚€ (ç´„5ï½10åˆ†)ã€‚ç„¦ã’ä»˜ã‹ãªã„ã‚ˆã†ã«æ™‚ã€…æ··ãœã‚‹ã€‚\\n5.  **ç››ã‚Šä»˜ã‘:**\\n    *   ã”é£¯ã‚’çš¿ã«ç››ã‚Šã€ã‚«ãƒ¬ãƒ¼ã‚’ã‹ã‘ã‚Œã°å®Œæˆã€‚\\n\\n**ç¾å‘³ã—ãä½œã‚‹ãŸã‚ã®ãƒã‚¤ãƒ³ãƒˆ**\\n\\n*   **ç‰ã­ãã‚’ã—ã£ã‹ã‚Šç‚’ã‚ã‚‹:** ç‰ã­ãã‚’é£´è‰²ã«ãªã‚‹ã¾ã§ç‚’ã‚ã‚‹ã“ã¨ã§ã€ã‚«ãƒ¬ãƒ¼ã«ç”˜ã¿ã¨ã‚³ã‚¯ãŒå‡ºã¾ã™ã€‚ç„¦ã’ä»˜ã‹ãªã„ã‚ˆã†ã«æ³¨æ„ã—ãªãŒã‚‰ã€ã˜ã£ãã‚Šç‚’ã‚ã¾ã—ã‚‡ã†ã€‚\\n*   **éš ã—å‘³ã‚’åŠ ãˆã‚‹:** ãƒãƒ§ã‚³ãƒ¬ãƒ¼ãƒˆã€ã‚³ãƒ¼ãƒ’ãƒ¼ã€ã‚½ãƒ¼ã‚¹ã€ãƒ¨ãƒ¼ã‚°ãƒ«ãƒˆã€ã‚±ãƒãƒ£ãƒƒãƒ—ãªã©ã‚’å°‘é‡åŠ ãˆã‚‹ã¨ã€å‘³ã«æ·±ã¿ãŒå¢—ã—ã¾ã™ã€‚\\n*   **ç…®è¾¼ã¿æ™‚é–“ã‚’èª¿æ•´ã™ã‚‹:** é‡èœã®ç¡¬ã•ã¯ãŠå¥½ã¿ã§èª¿æ•´ã—ã¦ãã ã•ã„ã€‚æŸ”ã‚‰ã‹ã‚ãŒå¥½ããªå ´åˆã¯ã€ç…®è¾¼ã¿æ™‚é–“ã‚’é•·ã‚ã«ã—ã¾ã—ã‚‡ã†ã€‚\\n*   **ãƒ«ãƒ¼ã®ç¨®é¡ã‚’é¸ã¶:** ç”˜å£ã€ä¸­è¾›ã€è¾›å£ãªã©ã€ãŠå¥½ã¿ã®è¾›ã•ã®ãƒ«ãƒ¼ã‚’é¸ã³ã¾ã—ã‚‡ã†ã€‚\\n*   **ãƒ­ãƒ¼ãƒªã‚¨ã‚’åŠ ãˆã‚‹:** ç…®è¾¼ã¿ã®éš›ã«ãƒ­ãƒ¼ãƒªã‚¨ã‚’1ï½2æšåŠ ãˆã‚‹ã¨ã€é¢¨å‘³ãŒè‰¯ããªã‚Šã¾ã™ã€‚\\n\\n**ã‚¢ãƒ¬ãƒ³ã‚¸ãƒ¬ã‚·ãƒ”**\\n\\n*   **ã‚·ãƒ¼ãƒ•ãƒ¼ãƒ‰ã‚«ãƒ¬ãƒ¼:** è‚‰ã®ä»£ã‚ã‚Šã«ã‚¨ãƒ“ã€ã‚¤ã‚«ã€ã‚¢ã‚µãƒªãªã©ã®ã‚·ãƒ¼ãƒ•ãƒ¼ãƒ‰ã‚’åŠ ãˆã¦ä½œã‚‹ã€‚\\n*   **ãƒã‚­ãƒ³ã‚«ãƒ¬ãƒ¼:** é¶è‚‰ã‚’ãƒ¨ãƒ¼ã‚°ãƒ«ãƒˆã¨ã‚¹ãƒ‘ã‚¤ã‚¹ã«æ¼¬ã‘è¾¼ã‚“ã§ã‹ã‚‰ç‚’ã‚ã‚‹ã¨ã€æœ¬æ ¼çš„ãªå‘³ã‚ã„ã«ãªã‚‹ã€‚\\n*   **ã‚­ãƒ¼ãƒã‚«ãƒ¬ãƒ¼:** ã²ãè‚‰ã‚’ä½¿ã£ã¦ã€æ°´åˆ†ã‚’é£›ã°ã—ãªãŒã‚‰ç‚’ã‚ã¦ä½œã‚‹ã€‚\\n*   **é‡èœã‚«ãƒ¬ãƒ¼:** è‚‰ã‚’ä½¿ã‚ãšã€ãƒŠã‚¹ã€ãƒ”ãƒ¼ãƒãƒ³ã€ã‚«ãƒœãƒãƒ£ãªã©ã€ãŠå¥½ã¿ã®é‡èœã‚’ãŸã£ã·ã‚Šå…¥ã‚Œã¦ä½œã‚‹ã€‚\\n\\n**æ³¨æ„ç‚¹**\\n\\n*   ã‚«ãƒ¬ãƒ¼ãƒ«ãƒ¼ã®ç¨®é¡ã«ã‚ˆã£ã¦ã€ä½œã‚Šæ–¹ã‚„æ°´ã®é‡ãŒç•°ãªã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚ãƒ«ãƒ¼ã®ç®±ã®èª¬æ˜ã‚’ã‚ˆãèª­ã‚“ã§ã‹ã‚‰ä½œã‚Šã¾ã—ã‚‡ã†ã€‚\\n*   ç„¦ã’ä»˜ãã‚„ã™ã„ã®ã§ã€ç…®è¾¼ã‚“ã§ã„ã‚‹é–“ã¯æ™‚ã€…æ··ãœã‚‹ã‚ˆã†ã«ã—ã¾ã—ã‚‡ã†ã€‚\\n*   ä½œã£ãŸã‚«ãƒ¬ãƒ¼ã¯ã€å†·è”µåº«ã§ä¿å­˜ã—ã€æ—©ã‚ã«é£Ÿã¹ãã‚‹ã‚ˆã†ã«ã—ã¾ã—ã‚‡ã†ã€‚\\n\\nä¸Šè¨˜ã‚’å‚è€ƒã«ã€ãœã²ç¾å‘³ã—ã„ã‚«ãƒ¬ãƒ¼ãƒ©ã‚¤ã‚¹ã‚’ä½œã£ã¦ã¿ã¦ãã ã•ã„ã€‚\\nä¸æ˜ãªç‚¹ãŒã‚ã‚Œã°ã€ãŠæ°—è»½ã«ã”è³ªå•ãã ã•ã„ã€‚'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt ='''\\\n",
    "ä»¥ä¸‹ã®æ–™ç†ã®ãƒ¬ã‚·ãƒ”ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚\n",
    "\n",
    "æ–™ç†å: \"\"\"\n",
    "{dish}\n",
    "\"\"\"\n",
    "'''\n",
    "\n",
    "def get_recipe(dish_name: str) -> str:\n",
    "    response = llm.invoke([\n",
    "        {\"role\": \"user\", \"content\": prompt.format(dish=dish_name)}\n",
    "    ])\n",
    "    return response.content\n",
    "\n",
    "get_recipe(\"ã‚«ãƒ¬ãƒ¼ãƒ©ã‚¤ã‚¹\")\n",
    "#prompt.format(dish=\"ç”°ä¸­\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d3b6a1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== åŸºæœ¬çš„ãªå‘¼ã³å‡ºã— ===\n",
      "ã“ã‚“ã«ã¡ã¯ï¼ä»Šæ—¥ã®æ°—åˆ†ã¯ã¨ã¦ã‚‚è‰¯ã„ã§ã™ï¼ ğŸ˜Š ã‚ãªãŸã¯ã„ã‹ãŒã§ã™ã‹ï¼Ÿ\n",
      "\n",
      "=== ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä»˜ã ===\n",
      "ã¸ã„ã€æ¯åº¦ï¼ã‚ã„ã¯ã€çš†ã•ã‚“ã®é ¼ã‚Šã«ãªã‚‹é–¢è¥¿å¼ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã‚„ï¼å›°ã£ãŸã“ã¨ã€çŸ¥ã‚ŠãŸã„ã“ã¨ã€ä½•ã§ã‚‚ã‹ã‚“ã§ã‚‚æ°—è»½ã«èã„ã¦ãªï¼ãŠã‚‚ã‚ã„è©±ã‚‚å¤§å¥½ãã‚„ã§ï¼\n"
     ]
    }
   ],
   "source": [
    "# ====================================\n",
    "# LangSmith ãƒˆãƒ¬ãƒ¼ã‚¹ä¾‹ï¼ˆlangchain-google-vertexaiä½¿ç”¨ï¼‰\n",
    "# ====================================\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # LangSmithã®è¨­å®šã‚’èª­ã¿è¾¼ã‚€\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Vertex AIçµŒç”±ã§Geminiã‚’ä½¿ç”¨\n",
    "llm_vertex = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash-001\",\n",
    "    project=\"dify-chatbot-469403\",\n",
    "    location=\"us-central1\"\n",
    ")\n",
    "\n",
    "# åŸºæœ¬çš„ãªä½¿ã„æ–¹\n",
    "response = llm_vertex.invoke(\"ã“ã‚“ã«ã¡ã¯ï¼ä»Šæ—¥ã®æ°—åˆ†ã¯ã©ã†ï¼Ÿ\")\n",
    "print(\"=== åŸºæœ¬çš„ãªå‘¼ã³å‡ºã— ===\")\n",
    "print(response.content)\n",
    "print()\n",
    "\n",
    "# ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å½¢å¼\n",
    "response2 = llm_vertex.invoke([\n",
    "    {\"role\": \"system\", \"content\": \"ã‚ãªãŸã¯é–¢è¥¿å¼ã§è©±ã™ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚\"},\n",
    "    {\"role\": \"user\", \"content\": \"è‡ªå·±ç´¹ä»‹ã—ã¦ãã ã•ã„\"}\n",
    "])\n",
    "print(\"=== ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä»˜ã ===\")\n",
    "print(response2.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "aaa14fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\"\"\"ä»¥ä¸‹ã®æ–™ç†ã®ãƒ¬ã‚·ãƒ”ã‚’è€ƒãˆã¦ãã ã•ã„ã€‚æ–™ç†å {dish}\"\"\"\n",
    "])\n",
    "\n",
    "prompt_value = prompt.invoke({\"dish\": \"ã‚«ãƒ¬ãƒ¼\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c114508b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== æ–‡å­—åˆ—ã¨ã—ã¦è¡¨ç¤º ===\n",
      "Human: ä»¥ä¸‹ã®æ–™ç†ã®ãƒ¬ã‚·ãƒ”ã‚’è€ƒãˆã¦ãã ã•ã„ã€‚æ–™ç†å ã‚«ãƒ¬ãƒ¼\n",
      "\n",
      "=== ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ã—ã¦è¡¨ç¤º ===\n",
      "[HumanMessage(content='ä»¥ä¸‹ã®æ–™ç†ã®ãƒ¬ã‚·ãƒ”ã‚’è€ƒãˆã¦ãã ã•ã„ã€‚æ–™ç†å ã‚«ãƒ¬ãƒ¼', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "# ChatPromptValue ã®ä¸­èº«ã‚’ç¢ºèªã™ã‚‹æ–¹æ³•\n",
    "print(\"=== æ–‡å­—åˆ—ã¨ã—ã¦è¡¨ç¤º ===\")\n",
    "print(prompt_value.to_string())\n",
    "\n",
    "print(\"\\n=== ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ã—ã¦è¡¨ç¤º ===\")\n",
    "print(prompt_value.to_messages())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e5dbd4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"ã‚ãªãŸã¯æ–™ç†ã®å°‚é–€å®¶ã§ã™ã€‚\"),\n",
    "    (\"human\", \"ä»¥ä¸‹ã®æ–™ç†ã®ãƒ¬ã‚·ãƒ”ã‚’è€ƒãˆã¦ãã ã•ã„ã€‚æ–™ç†å {dish}\")\n",
    "])\n",
    "\n",
    "prompt_value = prompt.invoke({\"dish\": \"ã‚ªãƒ ãƒ©ã‚¤ã‚¹\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "25d6d353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== æ–‡å­—åˆ—ã¨ã—ã¦è¡¨ç¤º ===\n",
      "System: ã‚ãªãŸã¯æ–™ç†ã®å°‚é–€å®¶ã§ã™ã€‚\n",
      "Human: ä»¥ä¸‹ã®æ–™ç†ã®ãƒ¬ã‚·ãƒ”ã‚’è€ƒãˆã¦ãã ã•ã„ã€‚æ–™ç†å ã‚ªãƒ ãƒ©ã‚¤ã‚¹\n"
     ]
    }
   ],
   "source": [
    "print(\"=== æ–‡å­—åˆ—ã¨ã—ã¦è¡¨ç¤º ===\")\n",
    "print(prompt_value.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "91dbef02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== æ–‡å­—åˆ—ã¨ã—ã¦è¡¨ç¤º ===\n",
      "System: ã‚ãªãŸã¯æ–™ç†ã®å°‚é–€å®¶ã§ã™ã€‚\n",
      "Human: ä»¥ä¸‹ã®æ–™ç†ã®ãƒ¬ã‚·ãƒ”ã‚’è€ƒãˆã¦ãã ã•ã„ã€‚æ–™ç†å ã‚ªãƒ ãƒ©ã‚¤ã‚¹\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "#from langchain_core.tools import tool\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"ã‚ãªãŸã¯æ–™ç†ã®å°‚é–€å®¶ã§ã™ã€‚\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\", optional=True),\n",
    "    (\"human\", \"ä»¥ä¸‹ã®æ–™ç†ã®ãƒ¬ã‚·ãƒ”ã‚’è€ƒãˆã¦ãã ã•ã„ã€‚æ–™ç†å {dish}\")\n",
    "])\n",
    "\n",
    "prompt_value = prompt.invoke({\"dish\": \"ã‚ªãƒ ãƒ©ã‚¤ã‚¹\"})\n",
    "print(\"=== æ–‡å­—åˆ—ã¨ã—ã¦è¡¨ç¤º ===\")\n",
    "print(prompt_value.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "23f82dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_value = prompt.invoke({\n",
    "    \"dish\": \"ã‚«ãƒ¬ãƒ¼\",\n",
    "    \"history\": [\n",
    "        (\"human\", \"ã‚«ãƒ¬ãƒ¼ã®ãƒ¬ã‚·ãƒ”ã‚’æ•™ãˆã¦\"),\n",
    "        (\"ai\", \"ã‚«ãƒ¬ãƒ¼ã®ãƒ¬ã‚·ãƒ”ã¯ç‰ã­ãã‚’ç‚’ã‚ã¦...\"),\n",
    "        (\"human\", \"è¾›ãã™ã‚‹ã«ã¯ï¼Ÿ\"),\n",
    "        (\"ai\", \"ã‚¹ãƒ‘ã‚¤ã‚¹ã‚’å¢—ã‚„ã™ã¨è¾›ããªã‚Šã¾ã™\"),\n",
    "        (\"human\", \"æ¿€è¾›ãŒå¥½ããªã‚“ã ã‚ˆ\"),\n",
    "    ]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7323fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== æ–‡å­—åˆ—ã¨ã—ã¦è¡¨ç¤º ===\n"
     ]
    }
   ],
   "source": [
    "print(\"=== æ–‡å­—åˆ—ã¨ã—ã¦è¡¨ç¤º ===\")\n",
    "print(llm.invoke(prompt_value).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb95eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# æº–å‚™\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash-001\",\n",
    "    project=\"dify-chatbot-469403\",\n",
    "    location=\"us-central1\"\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"ã‚ãªãŸã¯æ–™ç†ã®å°‚é–€å®¶ã§ã™\"),\n",
    "    (\"human\", \"{dish}ã®ãƒ¬ã‚·ãƒ”ã‚’æ•™ãˆã¦\")\n",
    "])\n",
    "\n",
    "# å®Ÿè¡Œï¼ˆãƒ‘ã‚¤ãƒ—ã§ã¤ãªãï¼‰\n",
    "chain = prompt | llm\n",
    "result = chain.invoke({\"dish\": \"ã‚«ãƒ¬ãƒ¼\"})\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb6aac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "\n",
    "\n",
    "class Recipe(BaseModel):\n",
    "  ingredients: list[str] = Field(description=\"æ–™ç†ã®ææ–™ã®ãƒªã‚¹ãƒˆ\")\n",
    "  steps: list[str] = Field(description=\"æ–™ç†ã®æ‰‹é †ã®ãƒªã‚¹ãƒˆ\")\n",
    "\n",
    "\n",
    "output_parser = PydanticOutputParser(pydantic_object=Recipe)\n",
    "\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "print(\"=== æ–‡å­—åˆ—ã¨ã—ã¦è¡¨ç¤º ===\")\n",
    "print(format_instructions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f5be2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# å‹å®šç¾©\n",
    "class Recipe(BaseModel):\n",
    "    ingredients: list[str] = Field(description=\"æ–™ç†ã®ææ–™ã®ãƒªã‚¹ãƒˆ\")\n",
    "    steps: list[str] = Field(description=\"æ–™ç†ã®æ‰‹é †ã®ãƒªã‚¹ãƒˆ\")\n",
    "\n",
    "# ãƒ‘ãƒ¼ã‚µãƒ¼\n",
    "output_parser = PydanticOutputParser(pydantic_object=Recipe)\n",
    "\n",
    "# LLM\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash-001\",\n",
    "    project=\"dify-chatbot-469403\",\n",
    "    location=\"us-central1\"\n",
    ")\n",
    "\n",
    "# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"ã‚ãªãŸã¯æ–™ç†ã®å°‚é–€å®¶ã§ã™ã€‚\"),\n",
    "    (\"human\", \"ä»¥ä¸‹ã®æ–™ç†ã®ãƒ¬ã‚·ãƒ”ã‚’è€ƒãˆã¦ãã ã•ã„ã€‚æ–™ç†å {dish}\\n\\n{format_instructions}\")\n",
    "])\n",
    "\n",
    "# ãƒã‚§ãƒ¼ãƒ³ï¼ˆoutput_parserã‚’è¿½åŠ ï¼‰\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "# å®Ÿè¡Œ\n",
    "result = chain.invoke({\n",
    "    \"dish\": \"ã‚ªãƒ ãƒ©ã‚¤ã‚¹\",\n",
    "    \"format_instructions\": output_parser.get_format_instructions()\n",
    "})\n",
    "\n",
    "print(result)\n",
    "# â†’ Recipe(ingredients=[...], steps=[...])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8ce32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "class Recipe(BaseModel):\n",
    "    ingredients: list[str] = Field(description=\"æ–™ç†ã®ææ–™ã®ãƒªã‚¹ãƒˆ\")\n",
    "    steps: list[str] = Field(description=\"æ–™ç†ã®æ‰‹é †ã®ãƒªã‚¹ãƒˆ\")\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash-001\",\n",
    "    project=\"dify-chatbot-469403\",\n",
    "    location=\"us-central1\"\n",
    ")\n",
    "\n",
    "# ã“ã‚Œã ã‘ã§OKï¼\n",
    "structured_llm = llm.with_structured_output(Recipe)\n",
    "result = structured_llm.invoke(\"ã‚ªãƒ ãƒ©ã‚¤ã‚¹ã®ãƒ¬ã‚·ãƒ”ã‚’æ•™ãˆã¦\")\n",
    "\n",
    "print(result.ingredients)\n",
    "print(result.steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d82eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash-001\",\n",
    "    project=\"dify-chatbot-469403\",\n",
    "    location=\"us-central1\"\n",
    ")\n",
    "\n",
    "# 1ã¤ç›®: æ–™ç†åã‚’è€ƒãˆã‚‹\n",
    "prompt1 = ChatPromptTemplate.from_template(\"{ingredient}ã‚’ä½¿ã£ãŸæ–™ç†åã‚’1ã¤ã ã‘ç­”ãˆã¦\")\n",
    "\n",
    "# 2ã¤ç›®: ãã®æ–™ç†ã®ãƒ¬ã‚·ãƒ”ã‚’è€ƒãˆã‚‹\n",
    "prompt2 = ChatPromptTemplate.from_template(\"{dish}ã®ãƒ¬ã‚·ãƒ”ã‚’æ•™ãˆã¦\")\n",
    "\n",
    "# ãƒã‚§ãƒ¼ãƒ³ã‚’ã¤ãªã’ã‚‹\n",
    "chain = (\n",
    "    prompt1\n",
    "    | llm\n",
    "    | StrOutputParser()  # â† ã“ã“ã§æ–‡å­—åˆ—ã«å¤‰æ›\n",
    "    | (lambda dish: {\"dish\": dish})  # â† æ¬¡ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«æ¸¡ã™\n",
    "    | prompt2\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "result = chain.invoke({\"ingredient\": \"åµ\"})\n",
    "print(result)\n",
    "# 1. \"åµ\"ã‚’ä½¿ã£ãŸæ–™ç†å â†’ \"ã‚ªãƒ ãƒ©ã‚¤ã‚¹\"\n",
    "# 2. \"ã‚ªãƒ ãƒ©ã‚¤ã‚¹\"ã®ãƒ¬ã‚·ãƒ” â†’ \"ææ–™ã¯...\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da292201",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash-001\",\n",
    "    project=\"dify-chatbot-469403\",\n",
    "    location=\"us-central1\"\n",
    ")\n",
    "\n",
    "# å‹å®šç¾©\n",
    "class Recipe(BaseModel):\n",
    "    date: list[str] = Field(description=\"æ—¥ä»˜\")\n",
    "    menue: list[str] = Field(description=\"æ–™ç†å\")\n",
    "    ingredients: list[str] = Field(description=\"æ–™ç†ã®ææ–™ã®ãƒªã‚¹ãƒˆ\")\n",
    "    steps: list[str] = Field(description=\"æ–™ç†ã®æ‰‹é †ã®ãƒªã‚¹ãƒˆ\")\n",
    "\n",
    "# 1ã¤ç›®: æ–™ç†åã‚’è€ƒãˆã‚‹\n",
    "prompt1 = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"ã‚ãªãŸã¯æ–™ç†ã®å°‚é–€å®¶ã§ã™ã€‚æ–™ç†åã ã‘ã‚’1ã¤ç­”ãˆã¦ãã ã•ã„ã€‚\"),\n",
    "    (\"human\", \"{ingredient}ã‚’ä½¿ã£ãŸæ–™ç†ã¯ï¼Ÿ\")\n",
    "])\n",
    "\n",
    "# 2ã¤ç›®: ãƒ¬ã‚·ãƒ”ã‚’æ§‹é€ åŒ–ã—ã¦å–å¾—\n",
    "prompt2 = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"ã‚ãªãŸã¯æ–™ç†ã®å°‚é–€å®¶ã§ã™ã€‚\"),\n",
    "    (\"human\", \"{dish}ã®ãƒ¬ã‚·ãƒ”ã‚’æ•™ãˆã¦\")\n",
    "])\n",
    "\n",
    "structured_llm = llm.with_structured_output(Recipe)\n",
    "\n",
    "# ãƒã‚§ãƒ¼ãƒ³\n",
    "chain = (\n",
    "    prompt1\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    | (lambda dish: {\"dish\": dish})  # â† è¾æ›¸ã«å¤‰æ›\n",
    "    | prompt2\n",
    "    | structured_llm\n",
    ")\n",
    "\n",
    "result = chain.invoke({\"ingredient\": \"åµ\"})\n",
    "print(result.date)\n",
    "print(result.menue)\n",
    "print(result.ingredients)\n",
    "print(result.steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c8b21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# RAGï¼ˆRetrieval-Augmented Generationï¼‰å…¥é–€\n",
    "# ====================================\n",
    "# è‡ªåˆ†ã®ãƒ‡ãƒ¼ã‚¿ã‚’æ¤œç´¢ã—ã¦ã€LLMã«å›ç­”ã•ã›ã‚‹ä»•çµ„ã¿\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# LLMæº–å‚™\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash-001\",\n",
    "    project=\"dify-chatbot-469403\",\n",
    "    location=\"us-central1\"\n",
    ")\n",
    "\n",
    "# ====================================\n",
    "# ç°¡æ˜“ç‰ˆ: ãƒ€ãƒŸãƒ¼ã®Retrieverã‚’ä½œæˆ\n",
    "# ====================================\n",
    "# æœ¬ç•ªã§ã¯ Vertex AI Search ã‚„ ChromaDB ã‚’ä½¿ã†\n",
    "# ã“ã“ã§ã¯ä»•çµ„ã¿ã‚’ç†è§£ã™ã‚‹ãŸã‚ã«ã€ç°¡å˜ãªæ¤œç´¢é–¢æ•°ã‚’ä½œæˆ\n",
    "\n",
    "# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ï¼ˆæœ¬ç•ªã§ã¯Vector Storeã«ä¿å­˜ï¼‰\n",
    "documents = {\n",
    "    \"ã‚Šã‚“ã”\": \"ã‚Šã‚“ã”ã¯é’æ£®çœŒãŒç”Ÿç”£é‡æ—¥æœ¬ä¸€ã§ã™ã€‚ç³–åº¦ãŒé«˜ãã€ãµã˜ã€ã¤ãŒã‚‹ã€ç‹æ—ãªã©ã®å“ç¨®ãŒã‚ã‚Šã¾ã™ã€‚æ—¬ã¯ç§‹ã‹ã‚‰å†¬ã§ã™ã€‚\",\n",
    "    \"ã¿ã‹ã‚“\": \"ã¿ã‹ã‚“ã¯å’Œæ­Œå±±çœŒã‚„æ„›åª›çœŒãŒæœ‰åãªç”£åœ°ã§ã™ã€‚ãƒ“ã‚¿ãƒŸãƒ³CãŒè±Šå¯Œã§ã€å†¬ã®å®šç•ªæœç‰©ã§ã™ã€‚\",\n",
    "    \"ãƒãƒŠãƒŠ\": \"ãƒãƒŠãƒŠã¯ãƒ•ã‚£ãƒªãƒ”ãƒ³ã‚„ã‚¨ã‚¯ã‚¢ãƒ‰ãƒ«ã‹ã‚‰ã®è¼¸å…¥ãŒå¤šã„ã§ã™ã€‚æ „é¤Šä¾¡ãŒé«˜ãã€æ‰‹è»½ã«é£Ÿã¹ã‚‰ã‚Œã‚‹æœç‰©ã§ã™ã€‚\",\n",
    "    \"ã¶ã©ã†\": \"ã¶ã©ã†ã¯å±±æ¢¨çœŒãŒæœ‰åã§ã™ã€‚å·¨å³°ã‚„ã‚·ãƒ£ã‚¤ãƒ³ãƒã‚¹ã‚«ãƒƒãƒˆãªã©ã®å“ç¨®ãŒã‚ã‚Šã¾ã™ã€‚ãƒ¯ã‚¤ãƒ³ã®åŸæ–™ã«ã‚‚ãªã‚Šã¾ã™ã€‚\",\n",
    "}\n",
    "\n",
    "def simple_retriever(query: str) -> str:\n",
    "    \"\"\"ç°¡æ˜“æ¤œç´¢: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’è¿”ã™\"\"\"\n",
    "    results = []\n",
    "    for keyword, content in documents.items():\n",
    "        if keyword in query or any(word in query for word in [\"æœç‰©\", \"ãƒ•ãƒ«ãƒ¼ãƒ„\", \"ç”£åœ°\"]):\n",
    "            results.append(f\"ã€{keyword}ã«ã¤ã„ã¦ã€‘\\n{content}\")\n",
    "\n",
    "    if results:\n",
    "        return \"\\n\\n\".join(results)\n",
    "    return \"è©²å½“ã™ã‚‹æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\"\n",
    "\n",
    "# ====================================\n",
    "# RAGãƒã‚§ãƒ¼ãƒ³ã®æ§‹ç¯‰\n",
    "# ====================================\n",
    "\n",
    "# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"ã‚ãªãŸã¯æœç‰©ã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®æ¤œç´¢çµæœã‚’å…ƒã«è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚æ¤œç´¢çµæœã«ãªã„æƒ…å ±ã¯ã€Œã‚ã‹ã‚Šã¾ã›ã‚“ã€ã¨ç­”ãˆã¦ãã ã•ã„ã€‚\"),\n",
    "    (\"human\", \"æ¤œç´¢çµæœ:\\n{context}\\n\\nè³ªå•: {question}\")\n",
    "])\n",
    "\n",
    "# ãƒã‚§ãƒ¼ãƒ³\n",
    "# {\"context\": ..., \"question\": ...} ã®å½¢ã§promptã«æ¸¡ã™\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": lambda x: simple_retriever(x),  # æ¤œç´¢å®Ÿè¡Œ\n",
    "        \"question\": lambda x: x                     # è³ªå•ã¯ãã®ã¾ã¾ï¼ˆã“ã‚ŒãŒä¸»æµï¼‰\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# ====================================\n",
    "# å®Ÿè¡Œä¾‹\n",
    "# ====================================\n",
    "print(\"=== RAGå®Ÿè¡Œä¾‹ ===\\n\")\n",
    "\n",
    "# è³ªå•1: ãƒ‡ãƒ¼ã‚¿ã«å«ã¾ã‚Œã‚‹æƒ…å ±\n",
    "question1 = \"ã‚Šã‚“ã”ã¨ã¿ã‹ã‚“ã®ç”£åœ°ã¯ã©ã“ï¼Ÿ\"\n",
    "print(f\"è³ªå•: {question1}\")\n",
    "print(f\"å›ç­”: {chain.invoke(question1)}\\n\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# è³ªå•2: ãƒ‡ãƒ¼ã‚¿ã«å«ã¾ã‚Œãªã„æƒ…å ±\n",
    "question2 = \"ã„ã¡ã”ã®ç”£åœ°ã¯ã©ã“ï¼Ÿ\"\n",
    "print(f\"è³ªå•: {question2}\")\n",
    "print(f\"å›ç­”: {chain.invoke(question2)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98478777",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  langchain_google_genai import ChatGoogleGenerativeAI\n",
    "client = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash-001\",\n",
    "    project=\"dify-chatbot-469403\",\n",
    "    location=\"us-central1\"\n",
    ")\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.runnables import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970a7ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# Runnable: RunnableLambda ã¨ @chain\n",
    "# ====================================\n",
    "# ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§é–¢æ•°ã‚’ä½¿ã†æ–¹æ³•\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.runnables import chain\n",
    "\n",
    "# ------------------------------------\n",
    "# æ–¹æ³•1: lambdaï¼ˆæœ€ã‚‚ä¸€èˆ¬çš„ï¼‰\n",
    "# ------------------------------------\n",
    "# ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å†…ã§ç›´æ¥æ›¸ã‘ã‚‹\n",
    "\n",
    "chain1 = (\n",
    "    (lambda x: x.upper())       # å¤§æ–‡å­—ã«å¤‰æ›\n",
    "    | (lambda x: f\"çµæœ: {x}\")  # æ–‡å­—åˆ—ã‚’æ•´å½¢\n",
    ")\n",
    "print(chain1.invoke(\"hello\"))  # â†’ \"çµæœ: HELLO\"\n",
    "\n",
    "# ------------------------------------\n",
    "# æ–¹æ³•2: RunnableLambdaï¼ˆæ˜ç¤ºçš„ã«æ›¸ãå ´åˆï¼‰\n",
    "# ------------------------------------\n",
    "# lambda ã¨åŒã˜ã ãŒã€ã‚ˆã‚Šæ˜ç¤ºçš„\n",
    "\n",
    "chain2 = (\n",
    "    RunnableLambda(lambda x: x.upper())\n",
    "    | RunnableLambda(lambda x: f\"çµæœ: {x}\")\n",
    ")\n",
    "print(chain2.invoke(\"hello\"))  # â†’ \"çµæœ: HELLO\"\n",
    "\n",
    "# ------------------------------------\n",
    "# æ–¹æ³•3: @chain ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ï¼ˆè¤‡é›‘ãªå‡¦ç†ã®å ´åˆï¼‰\n",
    "# ------------------------------------\n",
    "# é–¢æ•°ãŒè¤‡é›‘ãªå ´åˆã«ä½¿ã†ï¼ˆã‚ã¾ã‚Šä½¿ã‚ãªã„ï¼‰\n",
    "\n",
    "@chain\n",
    "def process_data(data):\n",
    "    \"\"\"è¤‡æ•°ã‚¹ãƒ†ãƒƒãƒ—ã®å‡¦ç†ã‚’1ã¤ã®é–¢æ•°ã«ã¾ã¨ã‚ã‚‹\"\"\"\n",
    "    step1 = data.upper()\n",
    "    step2 = f\"çµæœ: {step1}\"\n",
    "    return step2\n",
    "\n",
    "print(process_data.invoke(\"hello\"))  # â†’ \"çµæœ: HELLO\"\n",
    "\n",
    "# ------------------------------------\n",
    "# å®Ÿè·µ: ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§ã®ä½¿ç”¨ä¾‹\n",
    "# ------------------------------------\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash-001\",\n",
    "    project=\"dify-chatbot-469403\",\n",
    "    location=\"us-central1\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"{dish}ã®ãƒ¬ã‚·ãƒ”ã‚’æ•™ãˆã¦\")\n",
    "\n",
    "# lambda ã‚’ä½¿ã£ã¦æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã«è¾æ›¸ã‚’æ¸¡ã™\n",
    "chain_with_lambda = (\n",
    "    (lambda x: {\"dish\": x})  # æ–‡å­—åˆ— â†’ è¾æ›¸ã«å¤‰æ›\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# æ–‡å­—åˆ—ã§ç›´æ¥å‘¼ã¹ã‚‹\n",
    "result = chain_with_lambda.invoke(\"ã‚ªãƒ ãƒ©ã‚¤ã‚¹\")\n",
    "print(result[:100] + \"...\")  # æœ€åˆã®100æ–‡å­—ã ã‘è¡¨ç¤º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xzkwhluphfp",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# PythonåŸºç¤: ã‚¯ãƒ©ã‚¹ã¨ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹\n",
    "# ====================================\n",
    "# LangChainã‚’ç†è§£ã™ã‚‹ä¸Šã§å¿…è¦ãªPythonã®åŸºç¤çŸ¥è­˜\n",
    "\n",
    "# ------------------------------------\n",
    "# ã‚¯ãƒ©ã‚¹ = è¨­è¨ˆå›³\n",
    "# ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ = è¨­è¨ˆå›³ã‹ã‚‰ä½œã£ãŸå®Ÿä½“\n",
    "# ------------------------------------\n",
    "\n",
    "class Dog:\n",
    "    # __init__ = åˆæœŸåŒ–ï¼ˆã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆæ™‚ã«è‡ªå‹•å®Ÿè¡Œï¼‰\n",
    "    # å®šç¾©ã¯æ›¸ããŒã€å‘¼ã³å‡ºã—ã¯æ›¸ã‹ãªã„\n",
    "    def __init__(self, name):\n",
    "        self.name = name  # self = ã“ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹è‡ªä½“\n",
    "\n",
    "    # æ™®é€šã®ãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆè‡ªåˆ†ã§å‘¼ã¶ï¼‰\n",
    "    def bark(self):\n",
    "        return f\"{self.name}ãŒãƒ¯ãƒ³ï¼ã¨é³´ã„ãŸ\"\n",
    "\n",
    "# ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆ\n",
    "dog1 = Dog(\"ãƒãƒ\")   # â†’ __init__(self=æ–°ã—ã„ç®±, name=\"ãƒãƒ\") ãŒè‡ªå‹•å®Ÿè¡Œ\n",
    "dog2 = Dog(\"ã‚¿ãƒ­ã‚¦\")  # â†’ __init__(self=æ–°ã—ã„ç®±, name=\"ã‚¿ãƒ­ã‚¦\") ãŒè‡ªå‹•å®Ÿè¡Œ\n",
    "\n",
    "# ãƒ¡ã‚½ãƒƒãƒ‰å‘¼ã³å‡ºã—\n",
    "print(dog1.bark())  # â†’ \"ãƒãƒãŒãƒ¯ãƒ³ï¼ã¨é³´ã„ãŸ\"ï¼ˆself = dog1ï¼‰\n",
    "print(dog2.bark())  # â†’ \"ã‚¿ãƒ­ã‚¦ãŒãƒ¯ãƒ³ï¼ã¨é³´ã„ãŸ\"ï¼ˆself = dog2ï¼‰\n",
    "\n",
    "# ------------------------------------\n",
    "# selfã®ç†è§£\n",
    "# ------------------------------------\n",
    "# dog1.bark() ãŒå‘¼ã°ã‚Œã‚‹ã¨\n",
    "#   â†’ bark(self=dog1) ã¨ã—ã¦å®Ÿè¡Œã•ã‚Œã‚‹\n",
    "#   â†’ self.name ã¯ dog1.name = \"ãƒãƒ\"\n",
    "\n",
    "# ------------------------------------\n",
    "# __init__ ãŒãªã„ã‚¯ãƒ©ã‚¹ï¼ˆãƒ‡ãƒ¼ã‚¿ã‚’æŒãŸãªã„å ´åˆï¼‰\n",
    "# ------------------------------------\n",
    "class Calculator:\n",
    "    def add(self, a, b):\n",
    "        return a + b\n",
    "\n",
    "calc = Calculator()  # ä½•ã‚‚æ¸¡ã•ãªãã¦OK\n",
    "print(calc.add(1, 2))  # â†’ 3\n",
    "\n",
    "# ------------------------------------\n",
    "# @ï¼ˆãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ï¼‰\n",
    "# ------------------------------------\n",
    "# é–¢æ•°ã‚’ã€Œãƒ©ãƒƒãƒ—ã€ã—ã¦è¿½åŠ æ©Ÿèƒ½ã‚’ä»˜ã‘ã‚‹æ§‹æ–‡\n",
    "# LangChainã§ã¯ @tool ãŒã‚ˆãä½¿ã‚ã‚Œã‚‹\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"æŒ‡å®šã—ãŸéƒ½å¸‚ã®å¤©æ°—ã‚’å–å¾—ã™ã‚‹\"\"\"\n",
    "    return f\"{city}ã®å¤©æ°—ã¯æ™´ã‚Œã§ã™\"\n",
    "\n",
    "# @tool ã‚’ä»˜ã‘ã‚‹ã¨ .invoke() ã§å‘¼ã¹ã‚‹ã‚ˆã†ã«ãªã‚‹\n",
    "print(get_weather.invoke({\"city\": \"æ±äº¬\"}))  # â†’ \"æ±äº¬ã®å¤©æ°—ã¯æ™´ã‚Œã§ã™\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2211d432",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "# ä¸¦åˆ—å®Ÿè¡Œã®ãƒã‚§ãƒ¼ãƒ³ä¾‹\n",
    "\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash-001\",\n",
    "    project=\"dify-chatbot-469403\",\n",
    "    location=\"us-central1\"\n",
    ")\n",
    "\n",
    "# ææ–™ã‚’èããƒ—ãƒ­ãƒ³ãƒ—ãƒˆ\n",
    "prompt_ingredients = ChatPromptTemplate.from_template(\n",
    "    \"{dish}ã®ææ–™ã‚’3ã¤ã ã‘ç®‡æ¡æ›¸ãã§\"\n",
    ")\n",
    "\n",
    "# ã‚«ãƒ­ãƒªãƒ¼ã‚’èããƒ—ãƒ­ãƒ³ãƒ—ãƒˆ\n",
    "prompt_calories = ChatPromptTemplate.from_template(\n",
    "    \"{dish}ã®ã‚«ãƒ­ãƒªãƒ¼ã‚’æ•°å€¤ã ã‘ã§ç­”ãˆã¦\"\n",
    ")\n",
    "\n",
    "# TODO: ä¸¦åˆ—å®Ÿè¡Œã™ã‚‹ãƒã‚§ãƒ¼ãƒ³ã‚’ä½œæˆ\n",
    "chain = (\n",
    "    (lambda x: {\"dish\": x})\n",
    "    | RunnableParallel(\n",
    "        ingredients=prompt_ingredients | llm | StrOutputParser(),\n",
    "        calories=prompt_calories | llm | StrOutputParser(),\n",
    "    )\n",
    ")\n",
    "result = chain.invoke(\"ã‚«ãƒ¬ãƒ¼\")\n",
    "\n",
    "print(result[\"ingredients\"]),\n",
    "print(\"==========\")\n",
    "print(result[\"calories\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2650e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableBranch\n",
    "\n",
    "# æ¡ä»¶åˆ†å²ã®ãƒã‚§ãƒ¼ãƒ³ä¾‹\n",
    "def is_food_question(x):\n",
    "    dish = x[\"dish\"]  # â† ã“ã“ã‚’è¿½åŠ ã™ã‚‹ã ã‘\n",
    "    if \"ã‚«ãƒ¬ãƒ¼\" in dish or \"ä½œã‚Šæ–¹\" in dish or \"ãƒ¬ã‚·ãƒ”\" in dish:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash-001\",\n",
    "    project=\"dify-chatbot-469403\",\n",
    "    location=\"us-central1\"\n",
    ")\n",
    "\n",
    "# ææ–™ã‚’èããƒ—ãƒ­ãƒ³ãƒ—ãƒˆ\n",
    "prompt_ingredients = ChatPromptTemplate.from_template(\n",
    "    \"æ–™ç†ã«é–¢ã™ã‚‹æƒ…å ±ï¼š{dish}\"\n",
    ")\n",
    "\n",
    "\n",
    "# TODO: æ¡ä»¶åˆ†å²ã™ã‚‹ãƒã‚§ãƒ¼ãƒ³ã‚’ä½œæˆ\n",
    "chain = (\n",
    "    (lambda x: {\"dish\": x})\n",
    "    | RunnableBranch(\n",
    "        (is_food_question, prompt_ingredients | llm | StrOutputParser()),\n",
    "        lambda x: \"æ–™ç†ã«é–¢ã™ã‚‹è³ªå•ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚\"\n",
    "    )\n",
    ")\n",
    "result = chain.invoke(\"ã‚«ãƒ¬ãƒ¼ã®ä½œã‚Šæ–¹ã«ã¤ã„ã¦æ•™ãˆã¦\")\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d85a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash-001\",\n",
    "    project=\"dify-chatbot-469403\",\n",
    "    location=\"us-central1\"\n",
    ")\n",
    "\n",
    "# ææ–™ã‚’èããƒ—ãƒ­ãƒ³ãƒ—ãƒˆ\n",
    "prompt_dish = ChatPromptTemplate.from_template(\n",
    "    \"æ–™ç†ã«é–¢ã™ã‚‹æƒ…å ±ï¼š{dish}\"\n",
    ")\n",
    "\n",
    "\n",
    "# TODO: æ¡ä»¶åˆ†å²ã™ã‚‹ãƒã‚§ãƒ¼ãƒ³ã‚’ä½œæˆ\n",
    "chain = (\n",
    "    (lambda x: {\"dish\": x})\n",
    "    | RunnablePassthrough.assign(new_key=prompt_dish | llm | StrOutputParser())\n",
    ")\n",
    "result = chain.invoke(\"ã‚«ãƒ¬ãƒ¼ã®ä½œã‚Šæ–¹ã«ã¤ã„ã¦æ•™ãˆã¦\")\n",
    "\n",
    "print(result),\n",
    "print(\"==========\")\n",
    "\n",
    "\n",
    "chain2 = (\n",
    "    (lambda x: {\"dish\": x})| prompt_dish | llm | StrOutputParser()\n",
    ")\n",
    "result2 = chain2.invoke(\"ã‚«ãƒ¬ãƒ¼ã®ä½œã‚Šæ–¹ã«ã¤ã„ã¦æ•™ãˆã¦\")\n",
    "print(result2),\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f4e5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import chain\n",
    "from langchain_community.document_loaders import GitLoader\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash-001\",\n",
    "    project=\"dify-chatbot-469403\",\n",
    "    location=\"us-central1\"\n",
    ")\n",
    "\n",
    "def file_filter(file_path: str) -> bool:\n",
    "    \"\"\"ç‰¹å®šã®ãƒ•ã‚¡ã‚¤ãƒ«ã ã‘ã‚’å‡¦ç†ã™ã‚‹ãƒ•ã‚£ãƒ«ã‚¿é–¢æ•°\"\"\"\n",
    "    return file_path.endswith(\".md\")  # ä¾‹: .mdãƒ•ã‚¡ã‚¤ãƒ«ã ã‘å‡¦ç†\n",
    "\n",
    "loader = GitLoader(\n",
    "    clone_url=\"https://github.com/langchain-ai/langchain\",\n",
    "    repo_path=\"./langchain\",\n",
    "    branch=\"master\",\n",
    "    file_filter=file_filter\n",
    ")\n",
    "\n",
    "documents = loader.load()[:10]\n",
    "print(f\"èª­ã¿è¾¼ã‚“ã ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {len(documents)}\")\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Embedding\n",
    "embeddings = VertexAIEmbeddings(\n",
    "    model_name=\"text-embedding-004\",\n",
    "    project=\"dify-chatbot-469403\",\n",
    "    location=\"us-central1\"\n",
    ")\n",
    "\n",
    "# Vector Storeä½œæˆ\n",
    "db = Chroma.from_documents(\n",
    "    documents,\n",
    "    embedding=embeddings,\n",
    "    collection_name=\"langchain_docs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74eb7d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"ä»¥ä¸‹ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‚è€ƒã«è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚\"),\n",
    "    (\"human\", \"ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ:\\n{context}\\n\\nè³ªå•: {question}\")\n",
    "])\n",
    "\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"context\": retriever\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain.invoke(\"LangChainã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f571477b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    (lambda x: {\"question\": x})\n",
    "    | RunnablePassthrough.assign(context=retriever)\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cda178c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class QueryGenerationOutput(BaseModel):\n",
    "    queries: list[str] = Field(..., description=\"æ¤œç´¢ã‚¯ã‚¨ãƒªã®ãƒªã‚¹ãƒˆ\")\n",
    "\n",
    "query_generation_prompt = ChatPromptTemplate.from_messages([\"\"\"\\\n",
    "è³ªå•ã«å¯¾ã—ã¦ãƒ™ã‚¯ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã§æ¤œç´¢ã™ã‚‹ãŸã‚ã®é©åˆ‡ãªæ¤œç´¢ã‚¯ã‚¨ãƒªã‚’3ã¤ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚\n",
    "è·é›¢ãƒ™ãƒ¼ã‚¹ã®é¡ä¼¼æ€§æ¤œç´¢ã®é™ç•Œã‚’å…‹æœã™ã‚‹ãŸã‚ã«ã€\n",
    "ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã¦å¤šæ§˜ãªè¦³ç‚¹ã‹ã‚‰ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã™ã‚‹ã‚¯ã‚¨ãƒªã‚’ä½œæˆã—ã¾ã™ã€‚\n",
    "\n",
    "è³ªå•: {question}\n",
    "\"\"\"])\n",
    "\n",
    "query_generation_chain = (\n",
    "    query_generation_prompt\n",
    "    | llm.with_structured_output(QueryGenerationOutput)\n",
    "    | (lambda x: x.queries)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79692487",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "multi_query_rag_chain = (\n",
    "    {\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"queries\": query_generation_chain,\n",
    "    }\n",
    "    | (lambda x: {\n",
    "        \"question\": x[\"question\"],\n",
    "        \"context\": sum([retriever.invoke(q) for q in x[\"queries\"]], [])\n",
    "    })\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84c4e43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
